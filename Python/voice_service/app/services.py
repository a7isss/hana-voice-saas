# app/services.py - Voice Service Core Logic
# Handles model loading and voice processing operations

import os
import json
import logging
import time
from typing import Optional, Tuple, Dict, Any
from vosk import Model, KaldiRecognizer
from TTS.api import TTS

logger = logging.getLogger(__name__)

class VoiceServiceError(Exception):
    """Custom exception for voice service errors"""
    pass

class ModelLoadError(VoiceServiceError):
    """Exception raised when models fail to load"""
    pass

class ProcessingError(VoiceServiceError):
    """Exception raised when voice processing fails"""
    pass

class VoiceService:
    """Self-hosted voice processing service using Coqui XTTS and Vosk Arabic"""

    def __init__(self):
        logger.info("Initializing Voice Service with Coqui XTTS and Vosk...")
        self._load_models()
        self.model_health = {
            "vosk_arabic": False,
            "coqui_xtts": False,
            "last_check": time.time()
        }
        self._check_model_health()

    def _load_models(self):
        """Load both TTS and STT models with Railway-first download logic"""
        max_retries = 3
        retry_delay = 2
        is_railway = os.path.exists("/data")

        for attempt in range(max_retries):
            try:
                # Vosk Arabic STT Model - Railway deployment prioritizes downloads
                target_stt_path = "/data/models/stt/vosk-model-ar-0.22-linto-1.1.0"
                local_stt_path = "models/vosk-model-ar-0.22-linto-1.1.0"

                if is_railway:
                    # Railway deployment: Check volume first, then download
                    if os.path.exists(target_stt_path):
                        logger.info("âœ… Vosk model exists on Railway volume")
                        vosk_path = target_stt_path
                    else:
                        logger.info("â¬‡ï¸ Downloading Vosk model to Railway volume...")
                        if not self._download_vosk_model(target_stt_path):
                            raise FileNotFoundError(f"Failed to download Vosk model")
                        vosk_path = target_stt_path
                else:
                    # Development environment: Use local if available
                    vosk_path = local_stt_path if os.path.exists(local_stt_path) else target_stt_path
                    if not os.path.exists(vosk_path):
                        logger.info(f"â¬‡ï¸ Downloading Vosk model for development...")
                        if not self._download_vosk_model(vosk_path):
                            raise FileNotFoundError(f"Failed to download Vosk model")

                logger.info(f"Loading Vosk model from: {vosk_path} (attempt {attempt + 1})")
                self.vosk_model = Model(vosk_path)
                logger.info("âœ… Vosk Arabic model loaded successfully")

                # Coqui XTTS Model - Always prefer volume, download if needed
                target_tts_path = "/data/models/tts/tts_models--multilingual--multi-dataset--xtts_v2"
                local_tts_path = "models/tts/tts_models--multilingual--multi-dataset--xtts_v2"

                # Check if TTS target exists (already downloaded to volume)
                if os.path.exists(target_tts_path):
                    logger.info("âœ… Coqui XTTS model exists on volume")
                    try:
                        self.tts_model = TTS(model_path=target_tts_path, config_path=os.path.join(target_tts_path, "config.json"), gpu=False)
                        logger.info("âœ… Coqui XTTS model loaded from volume")
                    except Exception as e:
                        logger.warning(f"TTS volume model failed, trying download: {e}")
                        logger.info("â¬‡ï¸ Falling back to TTS download...")
                        self.tts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=False)
                        logger.info("âœ… Coqui XTTS model downloaded successfully")
                elif os.path.exists(local_tts_path):
                    # Use local model (development)
                    logger.info("âœ… Using local Coqui XTTS model")
                    self.tts_model = TTS(model_path=local_tts_path, config_path=os.path.join(local_tts_path, "config.json"), gpu=False)
                    logger.info("âœ… Coqui XTTS model loaded locally")
                else:
                    # Download fresh (Railway/production first deployment)
                    logger.info("â¬‡ï¸ Downloading Coqui XTTS model...")
                    self.tts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=False)
                    logger.info("âœ… Coqui XTTS model downloaded successfully")

                logger.info("ğŸ¯ Voice Service initialization complete!")
                return

            except Exception as e:
                logger.error(f"Failed to load voice models (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    logger.info(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    raise ModelLoadError(f"Voice model loading failed after {max_retries} attempts: {e}")

    def _download_vosk_model(self, target_path: str) -> bool:
        """Download Vosk model directly from Alphacephei"""
        import requests

        try:
            url = "https://alphacephei.com/vosk/models/vosk-model-ar-0.22-linto-1.1.0.zip"
            logger.info(f"Downloading Vosk from: {url}")

            # Create target directory
            os.makedirs(os.path.dirname(target_path), exist_ok=True)

            # Download ZIP file
            response = requests.get(url, stream=True, timeout=300)
            response.raise_for_status()

            zip_path = target_path + ".zip"
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0

            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0 and downloaded % (10 * 1024 * 1024) == 0:  # Progress every 10MB
                            progress = (downloaded / total_size) * 100
                            logger.info(".1f")

            # Extract ZIP
            logger.info("Extracting Vosk model...")
            import zipfile
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(os.path.dirname(target_path))

            # Clean up ZIP file
            os.remove(zip_path)

            if os.path.exists(target_path):
                logger.info("âœ… Vosk model download completed successfully")
                return True
            else:
                logger.error("âŒ Vosk model extraction failed")
                return False

        except Exception as e:
            logger.error(f"âŒ Vosk download failed: {e}")
            return False

    def _check_model_health(self) -> Dict[str, bool]:
        """Check if models are healthy and responsive"""
        try:
            # Test Vosk model
            try:
                test_recognizer = KaldiRecognizer(self.vosk_model, 16000)
                self.model_health["vosk_arabic"] = True
            except Exception as e:
                logger.error(f"Vosk model health check failed: {e}")
                self.model_health["vosk_arabic"] = False

            # Test TTS model
            try:
                # Quick test synthesis with default speaker
                test_output = os.path.join(os.getcwd(), "health_check_test.wav")

                # Try to get available speakers first
                try:
                    speakers = self.tts_model.speakers
                    if speakers and len(speakers) > 0:
                        speaker = speakers[0]  # Use first available speaker
                        logger.info(f"Using TTS speaker: {speaker}")
                    else:
                        speaker = None
                except:
                    speaker = None

                self.tts_model.tts_to_file(
                    text="Ø§Ø®ØªØ¨Ø§Ø±",
                    file_path=test_output,
                    language="ar",
                    speaker_wav=None,
                    speaker=speaker
                )

                if os.path.exists(test_output):
                    os.remove(test_output)
                    self.model_health["coqui_xtts"] = True
                else:
                    self.model_health["coqui_xtts"] = False
            except Exception as e:
                logger.error(f"TTS model health check failed: {e}")
                self.model_health["coqui_xtts"] = False

            self.model_health["last_check"] = time.time()
            logger.info(f"Model health check: {self.model_health}")
            return self.model_health

        except Exception as e:
            logger.error(f"Model health check failed: {e}")
            self.model_health["vosk_arabic"] = False
            self.model_health["coqui_xtts"] = False
            return self.model_health

    def get_health_status(self) -> Dict[str, Any]:
        """Get comprehensive health status of the voice service"""
        health = self._check_model_health()
        return {
            "status": "healthy" if all(health.values()) else "degraded",
            "models": health,
            "timestamp": time.time(),
            "version": "1.0.0"
        }

    # Model loading is handled in the constructor with retry logic - no duplicate method needed

    def speech_to_text(self, audio_bytes: bytes) -> str:
        """Convert audio bytes to Arabic text using Vosk with retry logic

        Args:
            audio_bytes: Raw PCM audio data (16-bit, mono, 16kHz expected)

        Returns:
            Transcribed Arabic text
        """
        max_retries = 2
        retry_delay = 0.5

        for attempt in range(max_retries):
            try:
                # Validate input
                if not audio_bytes or len(audio_bytes) < 100:
                    logger.warning(f"STT: Invalid audio data (size: {len(audio_bytes) if audio_bytes else 0})")
                    return ""

                # Check model health before processing
                if not self.model_health.get("vosk_arabic", False):
                    logger.warning("STT: Vosk model not healthy, attempting to reinitialize...")
                    self._check_model_health()
                    if not self.model_health.get("vosk_arabic", False):
                        raise ProcessingError("Vosk model is not available")

                # Create recognizer for this session
                recognizer = KaldiRecognizer(self.vosk_model, 16000)
                recognizer.SetWords(True)

                # Process audio in chunks (streaming)
                chunk_size = 4000
                audio_data = memoryview(audio_bytes)

                for i in range(0, len(audio_data), chunk_size):
                    chunk = audio_data[i:i+chunk_size]
                    if not recognizer.AcceptWaveform(bytes(chunk)):
                        logger.debug(f"STT: Partial result processing chunk {i//chunk_size + 1}")

                # Get final result
                final_result = json.loads(recognizer.FinalResult())
                text = final_result.get("text", "").strip()

                if text:
                    logger.info(f"ğŸ¤ STT: '{text}' (attempt {attempt + 1})")
                    return text
                else:
                    logger.debug(f"ğŸ¤ STT: No speech detected (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        continue
                    return ""

            except json.JSONDecodeError as e:
                logger.warning(f"STT: JSON decode error (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                return ""
            except Exception as e:
                logger.error(f"STT: Speech-to-text processing failed (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                return ""

        return ""

    def text_to_speech(self, text: str, language: str = "ar",
                      output_filename: str = "response.wav") -> Optional[str]:
        """Convert Arabic text to speech using Coqui XTTS with retry logic

        Args:
            text: Arabic text to synthesize
            language: Language code (default: "ar" for Arabic)
            output_filename: Output WAV filename

        Returns:
            Path to generated audio file, or None if failed
        """
        max_retries = 2
        retry_delay = 1.0

        for attempt in range(max_retries):
            try:
                # Validate input
                if not text.strip():
                    logger.warning("TTS: Empty text provided")
                    return None

                # Check model health before processing
                if not self.model_health.get("coqui_xtts", False):
                    logger.warning("TTS: Coqui model not healthy, attempting to reinitialize...")
                    self._check_model_health()
                    if not self.model_health.get("coqui_xtts", False):
                        raise ProcessingError("Coqui TTS model is not available")

                # Sanitize filename
                safe_filename = "".join(c for c in output_filename if c.isalnum() or c in "._-").strip()
                if not safe_filename.endswith('.wav'):
                    safe_filename += '.wav'

                output_path = os.path.join(os.getcwd(), safe_filename)

                logger.info(f"ğŸ”Š TTS: Generating audio for '{text[:50]}...' (attempt {attempt + 1})")

                # Generate speech with timeout protection
                start_time = time.time()

                # Try to get available speakers first
                try:
                    speakers = self.tts_model.speakers
                    if speakers and len(speakers) > 0:
                        speaker = speakers[0]  # Use first available speaker
                    else:
                        speaker = None
                except:
                    speaker = None

                self.tts_model.tts_to_file(
                    text=text,
                    file_path=output_path,
                    language=language,
                    speaker_wav=None,
                    speaker=speaker
                )
                generation_time = time.time() - start_time

                # Verify file was created and has reasonable size
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    if file_size > 1000:  # At least 1KB for valid audio
                        logger.info(f"âœ… TTS: Audio generated successfully: {output_path}")
                        logger.debug(f"   File size: {file_size} bytes, Generation time: {generation_time:.2f}s")
                        return output_path
                    else:
                        logger.warning(f"TTS: Generated file too small ({file_size} bytes), retrying...")
                        if os.path.exists(output_path):
                            os.remove(output_path)
                else:
                    logger.warning(f"TTS: Output file not created: {output_path}")

                # Retry if failed
                if attempt < max_retries - 1:
                    logger.info(f"TTS: Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5  # Progressive delay
                    continue
                else:
                    logger.error(f"âŒ TTS: Failed to generate audio after {max_retries} attempts")
                    return None

            except Exception as e:
                logger.error(f"TTS: Text-to-speech processing failed (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 1.5
                    continue
                return None

        return None

    def process_voice_interaction(self, input_audio: bytes,
                                questionnaire_context: dict = None) -> Tuple[str, Optional[str]]:
        """Complete voice processing pipeline: STT â†’ Logic â†’ TTS

        Args:
            input_audio: Raw audio bytes from client
            questionnaire_context: Context for questionnaire logic

        Returns:
            Tuple of (response_text, audio_file_path)
        """
        try:
            # Step 1: Speech to Text
            user_text = self.speech_to_text(input_audio)

            if not user_text:
                # No speech detected
                response_text = "Ù„Ù… Ø£Ø³Ù…Ø¹ Ø´ÙŠØ¦Ø§Ù‹ Ø¨ÙˆØ¶ÙˆØ­. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙƒÙ„Ù… Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŸ"
            else:
                # Step 2: Process questionnaire logic (replace old JSON-to-voice system)
                response_text = self._process_questionnaire_logic(user_text, questionnaire_context)

            # Step 3: Text to Speech
            audio_file = self.text_to_speech(response_text)
            session_id = questionnaire_context.get("session_id", "unknown") if questionnaire_context else "unknown"

            logger.info(f"ğŸ”„ Session {session_id}: Voice interaction processed")

            return response_text, audio_file

        except Exception as e:
            logger.error(f"Voice interaction processing failed: {e}")
            error_response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
            error_audio = self.text_to_speech(error_response)
            return error_response, error_audio

    def _process_questionnaire_logic(self, user_text: str,
                                   context: dict = None) -> str:
        """Replace the old JSON-to-voice system with intelligent questionnaire logic

        This functions analyses Arabic healthcare input and provides appropriate responses.
        Replaces the rigid JSON processing with natural language understanding.

        Args:
            user_text: Arabic speech transcription
            context: Session context with current question state

        Returns:
            Appropriate Arabic response for the healthcare questionnaire
        """

        # Healthcare keyword patterns in Arabic
        pain_keywords = ["Ø£Ù„Ù…", "ÙˆØ¬Ø¹", "Ø£Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø£Ù„Ù…", "Ø£ØªØ£Ù„Ù…", "dolor", "pain"]
        medicine_keywords = ["Ø¯ÙˆØ§Ø¡", "Ø£Ø¯ÙˆÙŠØ©", "Ø£ØªÙ†Ø§ÙˆÙ„", "Ø¯ÙˆØ§Ø¡", "medicine", "medication"]
        symptom_keywords = ["Ø£Ø¹Ø±Ø§Ø¶", "Ù…Ø´ÙƒÙ„Ø©", "Ù…Ø´Ø§ÙƒÙ„", "symptoms", "problems"]
        appointment_keywords = ["Ù…ÙˆØ¹Ø¯", "Ø²ÙŠØ§Ø±Ø©", "Ø·Ø¨ÙŠØ¨", "appointment", "doctor"]
        numbers_keywords = ["ØµÙØ±", "ÙˆØ§Ø­Ø¯", "Ø§Ø«Ù†Ø§Ù†", "Ø«Ù„Ø§Ø«Ø©", "Ø£Ø±Ø¨Ø¹Ø©", "Ø®Ù…Ø³Ø©",
                           "Ø³ØªØ©", "Ø³Ø¨Ø¹Ø©", "Ø«Ù…Ø§Ù†ÙŠØ©", "ØªØ³Ø¹Ø©", "Ø¹Ø´Ø±Ø©", "zero", "one", "two"]

        user_text_lower = user_text.lower()

        # Pain assessment logic
        if any(keyword in user_text_lower for keyword in pain_keywords):
            return "Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù„Ù… Ù…Ù† Ù  Ø¥Ù„Ù‰ Ù¡Ù ØŸ (Ù  Ù„Ø§ Ø£Ù„Ù… Ùˆ Ù¡Ù  Ø£Ù„Ù… Ø´Ø¯ÙŠØ¯)"

        # Medication inquiry
        elif any(keyword in user_text_lower for keyword in medicine_keywords):
            return "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ø§Ù„ØªÙŠ ØªØªÙ†Ø§ÙˆÙ„Ù‡Ø§ Ø­Ø§Ù„ÙŠØ§Ù‹ØŸ ÙŠØ±Ø¬Ù‰ Ø°ÙƒØ± Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø¬Ø±Ø¹Ø§Øª Ø¥Ù† Ø£Ù…ÙƒÙ†"

        # General symptom assessment
        elif any(keyword in user_text_lower for keyword in symptom_keywords):
            return "ÙŠØ±Ø¬Ù‰ ÙˆØµÙ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„ØªÙŠ ØªØ´Ø¹Ø± Ø¨Ù‡Ø§ Ø¨ØªÙØµÙŠÙ„ Ø£ÙƒØ«Ø± Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„"

        # Appointment-related
        elif any(keyword in user_text_lower for keyword in appointment_keywords):
            return "Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„Ø© Ù…ÙˆØ¹Ø¯ Ù…Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¨ØŸ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø°Ù„Ùƒ"

        # Number recognition for pain scale
        elif any(keyword in user_text_lower for keyword in numbers_keywords):
            # Extract numbers for pain scale validation
            for word in user_text_lower.split():
                if word in ["ØµÙØ±", "zero"]:
                    return "ÙÙ‡Ù…ØªØŒ Ù„Ø§ ØªØ´Ø¹Ø± Ø¨Ø£ÙŠ Ø£Ù„Ù…. Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ. Ù„Ø¯ÙŠÙƒ Ø£Ø³Ø¦Ù„Ø© Ø£Ø®Ø±Ù‰ØŸ"
                elif word in ["ÙˆØ§Ø­Ø¯", "one", "Ø§Ø«Ù†Ø§Ù†", "two", "Ø«Ù„Ø§Ø«Ø©", "three"]:
                    return "ÙÙ‡Ù…Øª Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù„Ù… Ø§Ù„Ø®ÙÙŠÙØ©. Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø±Øª Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨"
                elif word in ["Ø£Ø±Ø¨Ø¹Ø©", "four", "Ø®Ù…Ø³Ø©", "five", "six", "six"]:
                    return "Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù„Ù… Ù…ØªÙˆØ³Ø·Ø©. ÙŠÙÙ†ØµØ­ Ø¨Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø©"
                elif word in ["Ø³Ø¨Ø¹Ø©", "seven", "Ø«Ù…Ø§Ù†ÙŠØ©", "eight", "ØªØ³Ø¹Ø©", "nine", "Ø¹Ø´Ø±Ø©", "ten"]:
                    return "Ø£Ù„Ù… Ø´Ø¯ÙŠØ¯. ÙŠØ±Ø¬Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø·Ø¨ÙŠØ© ÙÙˆØ±Ø§Ù‹"

        # Default fallback responses
        elif len(user_text.split()) < 2:
            # Very short input
            return "ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙØµÙŠÙ„ Ø£ÙƒØ«Ø± ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„"

        else:
            # Generic response for other inputs
            return "Ø´ÙƒØ±Ø§Ù‹ Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØµÙ Ø­Ø§Ù„ØªÙƒ Ø§Ù„ØµØ­ÙŠØ© Ø¨ØªÙØµÙŠÙ„ Ø£ÙƒØ«Ø±ØŸ"
